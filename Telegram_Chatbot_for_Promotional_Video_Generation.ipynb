{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nehal12-hammam/Fruits-Detection-using-YOLO/blob/main/Telegram_Chatbot_for_Promotional_Video_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Install required packages (run this in your environment)\n",
        "!pip install gTTS\n",
        "!pip install diffusers transformers accelerate\n",
        "!pip install gradio\n",
        "!pip install openai-whisper\n",
        "!pip install clip\n",
        "!pip install nest_asyncio\n",
        "!pip install python-telegram-bot --upgrade\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install moviepy\n",
        "!pip install gdown\n",
        "!pip install ffmpeg-python\n",
        "!pip install langchain\n",
        "!pip install python-telegram-bot gtts moviepy diffusers torch clip whisper scikit-learn Pillow\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import asyncio\n",
        "import os\n",
        "import logging\n",
        "import requests  # To make API calls to Ollama\n",
        "from telegram import Update\n",
        "from gtts import gTTS\n",
        "from moviepy.editor import ImageClip, AudioFileClip, concatenate_videoclips\n",
        "from PIL import Image, ImageEnhance\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import clip\n",
        "import torch\n",
        "import whisper\n",
        "from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackContext\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Enable logging\n",
        "# Enable logging\n",
        "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__) #Corrected function name)\n",
        "\n",
        "# Load the Stable Diffusion model\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\")\n",
        "pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Create directories to save images\n",
        "output_image_dir = \"generated_images\"\n",
        "if not os.path.exists(output_image_dir):\n",
        "    os.makedirs(output_image_dir)\n",
        "\n",
        "# Global variables to store user inputs\n",
        "user_data = {}\n",
        "\n",
        "# Helper functions (unchanged)\n",
        "def resize_image(image, target_size=(512, 512)):\n",
        "    image.thumbnail(target_size, Image.LANCZOS)\n",
        "    return image\n",
        "\n",
        "def enhance_image(image):\n",
        "    enhancer = ImageEnhance.Sharpness(image)\n",
        "    return enhancer.enhance(1.5)\n",
        "\n",
        "def generate_images(description, num_images=5):\n",
        "    images = []\n",
        "    logger.info(f\"Generating {num_images} images for description: {description}\")\n",
        "    for i in range(num_images):\n",
        "        try:\n",
        "            output = pipe(description, num_inference_steps=100, guidance_scale=7.5, height=512, width=512)\n",
        "            if len(output.images) == 0:\n",
        "                logger.warning(f\"No images generated for iteration {i + 1}.\")\n",
        "                continue\n",
        "            image = enhance_image(resize_image(output.images[0].copy()))\n",
        "            image_path = os.path.join(output_image_dir, f\"generated_image_{i + 1}.png\")\n",
        "            image.save(image_path)\n",
        "            images.append(image_path)\n",
        "            logger.info(f\"Saved image: {image_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating image {i + 1}: {e}\")\n",
        "    return images\n",
        "\n",
        "def create_video_from_images(images, audio_path, audio_duration, fps=2):\n",
        "    num_frames = int(audio_duration / 3)\n",
        "    image_duration = 3.0\n",
        "    clips = [ImageClip(image).set_duration(image_duration) for image in images]\n",
        "    video_clip = concatenate_videoclips(clips, method=\"compose\")\n",
        "    video_clip.fps = fps  # Set FPS here\n",
        "\n",
        "    audio = AudioFileClip(audio_path)\n",
        "    video_clip = video_clip.set_audio(audio)\n",
        "    video_path = \"generated_promo_video.mp4\"\n",
        "    video_clip.write_videofile(video_path, codec='libx264', audio=True, fps=fps)  # Add fps parameter here\n",
        "    return video_path\n",
        "\n",
        "def generate_audio(description):\n",
        "    tts = gTTS(description, lang='en')\n",
        "    audio_path = \"description_audio.mp3\"\n",
        "    tts.save(audio_path)\n",
        "    return audio_path\n",
        "\n",
        "def get_audio_duration(audio_path):\n",
        "    audio = AudioFileClip(audio_path)\n",
        "    return audio.duration\n",
        "\n",
        "def generate_promotional_video(product_info):\n",
        "    description = (\n",
        "        f\"Introducing {product_info['name']}: {product_info['desc']}. \"\n",
        "        f\"Target audience: {product_info['audience']}. \"\n",
        "        f\"Key features include: {product_info['features']}. \"\n",
        "        f\"Visual style: {product_info['style']}.\"\n",
        "    )\n",
        "\n",
        "    logger.info(\"Generating audio for the description...\")\n",
        "    audio_path = generate_audio(description)\n",
        "    images = generate_images(description)\n",
        "\n",
        "    audio_duration = get_audio_duration(audio_path)\n",
        "    logger.info(f\"Audio duration: {audio_duration:.2f} seconds.\")\n",
        "\n",
        "    video_path = create_video_from_images(images, audio_path, audio_duration)\n",
        "    return video_path\n",
        "\n",
        "def transcribe_video(video_path):\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(video_path)\n",
        "\n",
        "    transcripts = []\n",
        "    for segment in result['segments']:\n",
        "        start = segment['start']\n",
        "        end = segment['end']\n",
        "        text = segment['text']\n",
        "        transcripts.append({'start': start, 'end': end, 'text': text})\n",
        "\n",
        "    return transcripts\n",
        "\n",
        "def search_transcription(transcripts, query):\n",
        "    matches = [segment for segment in transcripts if query.lower() in segment['text'].lower()]\n",
        "    return matches if matches else None\n",
        "\n",
        "def encode_text(description):\n",
        "    text = clip.tokenize([description]).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text)\n",
        "    return text_features.cpu().numpy()\n",
        "\n",
        "def encode_image(image_path):\n",
        "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "    return image_features.cpu().numpy()\n",
        "\n",
        "def calculate_cosine_similarity(text_features, image_features):\n",
        "    return cosine_similarity(text_features, image_features)\n",
        "\n",
        "def find_unrelated_images(description, images, threshold=0.3):\n",
        "    text_features = encode_text(description)\n",
        "    unrelated_images = []\n",
        "    for img_file in images:\n",
        "        image_features = encode_image(img_file)\n",
        "        similarity = cosine_similarity(text_features, image_features)[0][0]\n",
        "        if similarity < threshold:\n",
        "            unrelated_images.append(img_file)\n",
        "    return unrelated_images\n",
        "\n",
        "# Function to interact with the Ollama tool\n",
        "def generate_response_with_ollama(input_text):\n",
        "    response = requests.post(\"http://localhost:11434/v1/ollama/t5\", json={\"input\": input_text})\n",
        "    if response.status_code == 200:\n",
        "        return response.json()['output']\n",
        "    else:\n",
        "        logger.error(\"Error communicating with Ollama API\")\n",
        "        return \"I encountered an error while trying to get a response.\"\n",
        "\n",
        "# Start command handler\n",
        "async def start(update: Update, context: CallbackContext):\n",
        "    user_id = update.message.chat_id\n",
        "    user_data[user_id] = {}\n",
        "    await update.message.reply_text(\"Welcome! Let's start by telling me the Product Name.\")\n",
        "\n",
        "# Handle message flow to collect product details step by step\n",
        "async def handle_message(update: Update, context: CallbackContext):\n",
        "    user_id = update.message.chat_id\n",
        "    text = update.message.text.strip().lower()  # Normalize the text to lowercase\n",
        "\n",
        "    if user_id not in user_data:\n",
        "        user_data[user_id] = {}\n",
        "\n",
        "    if text == \"cancel\":\n",
        "        await update.message.reply_text(\"Goodbye!\")\n",
        "        user_data.pop(user_id, None)  # Remove the user data\n",
        "        return\n",
        "\n",
        "    # Ask for product details step by step\n",
        "    if 'product_name' not in user_data[user_id]:\n",
        "        user_data[user_id]['product_name'] = text\n",
        "        await update.message.reply_text(\"Got it! Now tell me about the Product Description.\")\n",
        "    elif 'product_desc' not in user_data[user_id]:\n",
        "        user_data[user_id]['product_desc'] = text\n",
        "        await update.message.reply_text(\"Who is the Target Audience?\")\n",
        "    elif 'target_audience' not in user_data[user_id]:\n",
        "        user_data[user_id]['target_audience'] = text\n",
        "        await update.message.reply_text(\"Great! What are the Key Features of your product?\")\n",
        "    elif 'key_features' not in user_data[user_id]:\n",
        "        user_data[user_id]['key_features'] = text\n",
        "        await update.message.reply_text(\"What Visual Style do you prefer?\")\n",
        "    elif 'visual_style' not in user_data[user_id]:\n",
        "        user_data[user_id]['visual_style'] = text\n",
        "\n",
        "        # Summary and generating video\n",
        "        summary = (\n",
        "            f\"*Product Name:* {user_data[user_id]['product_name']}\\n\"\n",
        "            f\"*Description:* {user_data[user_id]['product_desc']}\\n\"\n",
        "            f\"*Target Audience:* {user_data[user_id]['target_audience']}\\n\"\n",
        "            f\"*Key Features:* {user_data[user_id]['key_features']}\\n\"\n",
        "            f\"*Visual Style:* {user_data[user_id]['visual_style']}\\n\"\n",
        "        )\n",
        "        await update.message.reply_text(f\"Here is a summary of your inputs:\\n{summary}\")\n",
        "        await update.message.reply_text(\"Generating promotional video...\")\n",
        "\n",
        "        # Generate promotional video\n",
        "        video_path = generate_promotional_video({\n",
        "            'name': user_data[user_id]['product_name'],\n",
        "            'desc': user_data[user_id]['product_desc'],\n",
        "            'audience': user_data[user_id]['target_audience'],\n",
        "            'features': user_data[user_id]['key_features'],\n",
        "            'style': user_data[user_id]['visual_style']\n",
        "        })\n",
        "\n",
        "        # Send the generated video\n",
        "        with open(video_path, 'rb') as video_file:\n",
        "            await update.message.reply_video(video=video_file, caption=\"Here is your generated promotional video!\")\n",
        "\n",
        "        # Transcribe the generated video for searching later\n",
        "        user_data[user_id]['transcript'] = transcribe_video(video_path)\n",
        "\n",
        "        # Offer the user options after video generation\n",
        "        await update.message.reply_text(\n",
        "            \"Would you like to search in the video transcript?\\n\"\n",
        "            \"Use /search <keyword> to search.\\n\"\n",
        "            \"Or you can upload another video to search using /search_video.\\n\"\n",
        "\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        await update.message.reply_text(\"It seems you have already provided all necessary information. If you'd like to start over, just type 'cancel'.\")\n",
        "\n",
        "# Command to handle search in the video transcript\n",
        "async def handle_search(update: Update, context: CallbackContext):\n",
        "    if len(update.message.text.split(\"/search \", 1)) < 2:\n",
        "        await update.message.reply_text(\"Please provide a search query after the command.\")\n",
        "        return\n",
        "\n",
        "    query = update.message.text.split(\"/search \", 1)[1]\n",
        "    user_id = update.message.chat_id\n",
        "    transcript_segments = user_data[user_id].get('transcript', None)\n",
        "\n",
        "    if not transcript_segments:\n",
        "        await update.message.reply_text(\"There is no transcribed video to search. Please provide a video first.\")\n",
        "        return\n",
        "\n",
        "    matches = search_transcription(transcript_segments, query)\n",
        "    if matches:\n",
        "        result_text = \"\\n\".join([f\"Found at {segment['start']}s: {segment['text']}\" for segment in matches])\n",
        "        await update.message.reply_text(f\"Results for '{query}':\\n{result_text}\")\n",
        "    else:\n",
        "        await update.message.reply_text(f\"No matches found for '{query}'.\")\n",
        "\n",
        "# Command to handle video uploading and transcription\n",
        "async def handle_video(update: Update, context: CallbackContext):\n",
        "    video = update.message.video  # Get the video sent by the user\n",
        "    if not video:\n",
        "        await update.message.reply_text(\"Please send a valid video for transcription.\")\n",
        "        return\n",
        "\n",
        "    await update.message.reply_text(\"Transcribing the video, please wait...\")\n",
        "\n",
        "    # Download the video\n",
        "    video_file = await context.bot.get_file(video.file_id)\n",
        "    video_path = \"user_video.mp4\"\n",
        "    await video_file.download_to_drive(video_path)\n",
        "\n",
        "    # Transcribe the video\n",
        "    transcript_segments = transcribe_video(video_path)\n",
        "    user_id = update.message.chat_id\n",
        "    user_data[user_id]['transcript'] = transcript_segments\n",
        "\n",
        "    await update.message.reply_text(\"Video transcription completed. You can now search by sending /search <keyword>.\")\n",
        "\n",
        "# Command to find unrelated images\n",
        "# async def find_unrelated(update: Update, context: CallbackContext):\n",
        "#     user_id = update.message.chat_id\n",
        "\n",
        "#     # Initialize user data if not present\n",
        "#     if user_id not in user_data:\n",
        "#         user_data[user_id] = {'product_desc': None, 'generated_images': []}\n",
        "\n",
        "#     await update.message.reply_text(\"Please provide the product description.\")\n",
        "\n",
        "# # Handle finding unrelated images\n",
        "# async def handle_find_unrelated_message(update: Update, context: CallbackContext):\n",
        "#     user_id = update.message.chat_id\n",
        "#     text = update.message.text\n",
        "#     message = update.message\n",
        "\n",
        "#     if user_id not in user_data:\n",
        "#         user_data[user_id] = {'product_desc': None, 'generated_images': []}\n",
        "\n",
        "#     if user_data[user_id]['product_desc'] is None:\n",
        "#         user_data[user_id]['product_desc'] = text\n",
        "#         await update.message.reply_text(\n",
        "#             \"Thank you! You provided the following description:\\n\"\n",
        "#             f\"{text}\\n\\n\"\n",
        "#             \"Now, please upload the images to check against the description. You can send multiple images.\"\n",
        "#         )\n",
        "#     else:\n",
        "#         # Check if the message contains images\n",
        "#         if message.photo:\n",
        "#             image_file = await context.bot.get_file(message.photo[-1].file_id)\n",
        "#             image_path = f\"downloaded_image_{user_id}_{len(user_data[user_id]['generated_images'])}.jpg\"\n",
        "#             await image_file.download(image_path)\n",
        "#             user_data[user_id]['generated_images'].append(image_path)\n",
        "#             await update.message.reply_text(\"Image received. You can upload more images. Type 'done' when finished uploading images.\")\n",
        "#         elif text.lower() == 'done':\n",
        "#             if not user_data[user_id]['generated_images']:\n",
        "#                 await update.message.reply_text(\"No images uploaded. Please upload at least one image.\")\n",
        "#                 return\n",
        "\n",
        "#             description = user_data[user_id]['product_desc']\n",
        "#             images = user_data[user_id]['generated_images']\n",
        "\n",
        "#             # Call the function to find unrelated images\n",
        "#             unrelated_images = find_unrelated_images(description, images)\n",
        "\n",
        "#             if unrelated_images:\n",
        "#                 await update.message.reply_text(\"The following images are unrelated to the description:\")\n",
        "#                 for img_path in unrelated_images:\n",
        "#                     with open(img_path, 'rb') as img_file:\n",
        "#                         await update.message.reply_photo(photo=img_file)\n",
        "#             else:\n",
        "#                 await update.message.reply_text(\"All images are related to the description!\")\n",
        "\n",
        "#             # Reset user data after completion\n",
        "#             user_data[user_id] = {'product_desc': None, 'generated_images': []}\n",
        "#         else:\n",
        "#             await update.message.reply_text(\"Please upload an image or type 'done' when finished uploading images.\")\n",
        "\n",
        "def main():\n",
        "    nest_asyncio.apply()\n",
        "    application = Application.builder().token(\"7589464561:AAG1tdLLQ1KduvEUoZcEzNgDQrpMBm9YSiE\").build()\n",
        "\n",
        "    application.add_handler(CommandHandler(\"start\", start))\n",
        "    application.add_handler(CommandHandler(\"search\", handle_search))\n",
        "    application.add_handler(CommandHandler(\"search_video\", handle_video))\n",
        "    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))\n",
        "    application.add_handler(MessageHandler(filters.VIDEO, handle_video))\n",
        "    # application.add_handler(CommandHandler(\"find_unrelated\", find_unrelated))\n",
        "    # application.add_handler(MessageHandler(filters.TEXT & filters.PHOTO, handle_find_unrelated_message))\n",
        "\n",
        "    application.run_polling()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "V8_s7nBZmwqN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}